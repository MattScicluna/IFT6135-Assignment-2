%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size 

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{mathtools}
\usepackage{sectsty} % Allows customizing section commands
%\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\usepackage{bbm}
\usepackage{graphicx}
\usepackage{xcolor} % For color
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage{tikz} % For graphs
\usetikzlibrary{positioning}
\usetikzlibrary{calc}

\usepackage{enumerate} % For lettered enumeration

\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[noend]{algpseudocode} % for pseudocode

% commands
\newcommand{\Ex}[2]{\mathbb{E}_{#1}\left\{#2\right\}}
\newcommand{\dP}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\Var}[1]{Var\left\{#1\right\}}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Assignment Two \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{
	Matthew C.~Scicluna\\
	D\'epartement d'Informatique et de Recherche Op\'erationnelle\\
	Universit\'e de Montr\'eal\\
	Montr\'eal, QC H3T 1J4 \\
	\texttt{matthew.scicluna@umontreal.ca}
}


\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section{Convolutions}
We compute the full valid and same convolution with kernel flipping for the following matrices: $[1, 2, 3, 4] * [1, 0, 2]$

\begin{itemize}
	\item The valid convolution is: $[1\cdot2 + 2\cdot0 + 3\cdot1, \ 2\cdot2 + 3\cdot0 + 4\cdot1] = [5,8]$
	\item Likewise the same convolution is: $[0, 1, 2, 3, 4, 0] * [1, 0, 2] = [2,5,8,6]$
	\item Finally the full convolution is: $[0, 0, 1, 2, 3, 4, 0, 0] * [1, 0, 2] = [1,2,5,8,6,8]$
\end{itemize}

\section{Convolutional Neural Networks}
Consider a $3$-layer CNN. We are given an input of size $3\times256\times256$. The first layer contains $64$ $8\times8$ kernels using a stride of $2$ and no padding. The shape of its output is $64\times125\times125$ using relationship 6 from \cite{journals/corr/DumoulinV16}: 
$$\text{output length}=\floor*{\frac{256+2\cdot0-8}{2}}+1=125$$
The second layer subsamples this using $5\times5$ non-overlapping max pooling. It is easy to see that the size of its output is $64\times25\times25$, since $\frac{125}{5}=25$. The final layer convolves $128$ $4\times4$ kernels with a stride of $1$ and a zero-padding of size $1$ on each border. Using the formula we have that $\floor*{\frac{25+2\cdot1-4}{1}}+1=24$, and so the output of the last layer has shape $128\times24\times24$.
\begin{enumerate}[(a)]
	\item The output of the last layer will be of size: $128\times24\times24 = 73725$
	\item Ignoring biases, we would need $64\times25\times25\times128 = 5120000$ weights
\end{enumerate}

\section{Kernel configuration for CNNs}
We are given an input shape of $3\times64\times64$ and the output shape
is $64\times32\times32$ for a convolutional layer.
\begin{enumerate}[(a)]
	\item Assuming no dilation and kernel size of $8\times8$, we can solve for the stride length $s$ and the padding $p$ by solving the relationship with the given kernel size (setting $s=2$ for simplicity):
	\begin{align*}
	\floor*{\frac{64+2\cdot p-8}{2}}+1&=32\\
	32+p-4+1&=32\\
	p&=3
	\end{align*}
	Setting $3$ padding with $2$ stride satisfies the convolution dimensions. Assuming dilatation $d=6$ and stride of $s=2$, we can use relationship 15 from \cite{journals/corr/DumoulinV16} to get:
	\begin{align*}
		\floor*{\frac{64+2\cdot p-k-(k-1)(6-1)}{2}}+1&=32\\
		\floor*{\frac{69+2\cdot p-6\cdot k}{2}}&=31\\
	\end{align*}
	This is satisfied when $69+2\cdot p-6\cdot k = 63$. We simplify further to get $2p-6k+6=0$, for which one possible solution is: $p=3$, $k=2$. Therefore, setting padding to be $3$ and kernel size $2\times2$ satisfies the convolution dimensions.
	\item Given an input shape of $64\times32 \times32$ and the output shape
	is $64\times8 \times8$ a configuration assuming no overlapping of pooling windows or padding would have kernel size $4$ and stride $1$. This is easily seen since $\frac{32}{8}=4$.
	\item Without any padding and given input shape $64\times32 \times32$ and kernel of size $8\times8$ and stride $4$ we can use the relation to get:
	$$\text{output length}=\floor*{\frac{32+2\cdot0-8}{4}}+1=7$$
	And so the output size would be $7\times7$.
	\item We are given input shape $64\times8 \times8$ and output $128\times4 \times4$
	\begin{enumerate}[(i)]
		\item Assuming no padding and no dilation and using the relation above, we can easily solve to get kernel size $4$ and stride $2$.
		\item Assuming dilatation of $1$ and padding of $2$, kernel size $6$ and stride $2$ satisfies the input/output dimensions.
		\item Assuming padding of $1$ and no dilatation, kernel size $4$ and stride $2$ satisfies the input/output dimensions.
	\end{enumerate}
\end{enumerate}

\section{Dropout as weight decay}
	We consider a linear regression problem with input data $X\in\mathbb{R}^{n\times d}$, weights $w\in R^{d\times1}$ and
	and targets $y\in R^{n\times1}$. We also suppose that dropout is being applied to the input units with probability $p$.
\begin{enumerate}[(a)]
	\item We can let $\tilde{X}=P\odot X$ where $P_{ij}\sim Bernoulli(p)$
	\item The cost function of this would be
	\begin{align*}
	\Ex{P}{\| y-\tilde{X}w\|^2} &= \Ex{P}{(y-\tilde{X}w)^T(y-\tilde{X}w) } \\
	&= y^Ty-2w^T\Ex{P}{\tilde{X}^T}y+\Ex{P}{w^T\tilde{X}^T\tilde{X}w}\\
	&= y^Ty-2pw^TX^Ty+ \Bigl( p^2 (Xw)^T(Xw) - p^2 (Xw)^T(Xw) \Bigr) + \Ex{P}{w^T\tilde{X}^T\tilde{X}w}\\
	&= \| y-pXw\|^2 + \Ex{P}{w^T\tilde{X}^T\tilde{X}w} - p^2 (Xw)^T(Xw)\\
	&= \| y-pXw\|^2 + \Ex{P}{w^T\tilde{X}^T\tilde{X}w} - \Ex{P}{(\tilde{X}w)^T(\tilde{X}w)}\\
	&= \| y-pXw\|^2 + w^T\left(\Ex{P}{\tilde{X}^T\tilde{X}} - \Ex{P}{\tilde{X}}^T\Ex{p}{\tilde{X}}\right)w
	\end{align*}
	We evaluate the matrix in the rightmost term:
	\begin{align*}
	\Ex{P}{\tilde{X}^T\tilde{X}} - \Ex{P}{\tilde{X}}^T\Ex{p}{\tilde{X}} &= \left[ \sum_k \Ex{}{p_{ki}p_{kj}}x_{ki}x_{kj}-\Ex{}{p_{ki}}\Ex{}{p_{kj}}x_{ki}x_{kj} \right]_{ij}\\
	&= \sum_k Cov(p_{ki},p_{kj})x_{ki}x_{kj}\\
	&=
	\begin{cases}
	\sum_k p(1-p)x_{ki}^2
	& \text{if $i=j$}\\
	0 & \text{o.w.}
	\end{cases}\\
	&= Diag(X^TX)p(1-p)
	\end{align*}
	Inserting this into the cost function gives us:
	\begin{align*}
	\Ex{P}{\| y-\tilde{X}w\|^2} &= \| y-pXw\|^2 + p(1-p) w^TDiag(X^TX)w\\
	&= \| y-pXw\|^2 + p(1-p) \|\Gamma w\|^2
	\end{align*}
	Where $\Gamma = Diag(X^TX)^{\frac{1}{2}}$
	\item We show that applying dropout to the linear regression problem can be seen as using L2 regularization in the loss function. Let $\tilde{w}Ì„=pw$. The optimal value of the cost function is:
	\begin{align*}
		\dP{}{\tilde{w}}\left( \| y-X\tilde{w}\|^2 + \frac{1-p}{p} \|\Gamma \tilde{w}\|^2\right) &= -2X^Ty + 2X^TX\tilde{w} + 2\frac{1-p}{p}\Gamma^2\tilde{w}
	\end{align*}
	And setting this to zero yields:
	\begin{align*}
		&\frac{1-p}{p}\Gamma^2\tilde{w} + X^TX\tilde{w} =  X^Ty\\
		&\Rightarrow  \left(\frac{1-p}{p}\Gamma^2 + X^TX\right)\tilde{w} = X^Ty\\
		&\Rightarrow \tilde{w} = \left(\lambda\Gamma^2 + X^TX\right)^{-1}X^Ty
	\end{align*}
	where $\lambda=\frac{1-p}{p}$. Notice that the solution to the regularized least squares problem is identical except the $\lambda\Gamma^2$ term is replaced by $\lambda I$. In dropout, the $\Gamma$ term adds additional cost to weights which are in directions where the data varies, wheras in ordinary L2 regularized least squares, the directions are penalized by the same amount.
\end{enumerate}

\section{Dropout as Geometric Ensemble}

We show that weight scaling with a factor of $0.5$ corresponds exactly to the inference of a conditional probability distribution proportional to the geometric mean over all dropout masks:
$$ p_{\text{ens}}(y=j|v) \propto \left( \prod_{i=1}^N \hat{y}_j^{(i)} \right)^{\frac{1}{N}} $$
Where $N$ is the number of dropout masks, $\hat{y}_j^{(i)}=softmax\left(W^T(m_i\odot v)+b\right)_j$ and $m_i$ is a dropout mask configuration, for which there are $N$ of. We expand the geometric mean:
\begin{align*}
p_{\text{ens}}(y=j|v) &\propto \left( \prod_{i=1}^N softmax\left(W^T(m_i\odot v)+b\right)_j\right)^{\frac{1}{N}}\\
&= \left(\prod_{i=1}^N \frac{\exp\left\{W^T(m_i\odot v)+b\right\}_j}{\sum_{j'}\exp\left\{W^T(m_i\odot v)+b\right\}_{j'}}\right)^\frac{1}{N}\\
&= \frac{\left(\prod_{i=1}^N \exp\left\{W^T(m_i\odot v)+b\right\}_j \right)^{\frac{1}{N}}}{\left(\prod_{i=1}^N\sum_{j'}\exp\left\{W^T(m_i\odot v)+b\right\}_{j'}\right)^{\frac{1}{N}}}\\
&\propto \left(\prod_{i=1}^N \exp\left\{W^T(m_i\odot v)+b\right\}_j \right)^{\frac{1}{N}}\\
&=\exp\left\{\frac{1}{N}\sum_{i=1}^N W^T(m_i\odot v)+b\right\}_j\\
&=\exp\left\{\frac{1}{2}W^Tv+b\right\}_j
\end{align*}

\section{Normalization}
We investigate Weight Normalization (WN). We decouple the weight vector into two terms : 
$$ w = \frac{g}{\|u\|}u $$
where $g\in\mathbb{R}$ is a scaling factor. Doing so has similar effects as implementing Batch Normalization (BN), but has a lower computational overhead.
\begin{enumerate}[(a)]
	\item We consider the simplest model, where we only have one single output layer conditioned on one input feature $x$. Additionally, we assume $\Ex{}{x}=0, \Var{x}=1$. We show that in this simple case WN is equivalent to BN (ignoring the learned scale and shift terms) that normalizes the linearly transformed feature $a=w^Tx+b$. From 8.35 of \cite{Goodfellow-et-al-2016} we have that:
	\begin{align*}
	BN(a) = \frac{a-\Ex{}{a}}{\sqrt{\Var{a}}} = \frac{w^Tx}{\|w\|}
	\end{align*}
	since $\Ex{}{w^Tx}=w^T\Ex{}{x}=0$ and $\Var{w^Tx}=w^T\Var{x}w=w^Tw=\|w\|^2$.
	Ignoring the scale and shift terms, we see that this is equivalent to weight normalization.
	\item Show that the gradient of a loss function L with respect to the new parameters u can be expressed in the form $sW^*\nabla_w L$, where $s$ is a scalar and $W^*$ is the orthogonal complement projection matrix. We compute the gradient. Using the multivariate chain rule:
	\begin{align*}
		\nabla_u L &= \nabla_u w \nabla_w L\\
		&= g \nabla_u \frac{u}{\|u\|} \nabla_w L\\
		&= \frac{g}{\|u\|} \left( I- \frac{uu^T}{\|u\|^2} \right) \nabla_w L
	\end{align*}
	Since 
	\begin{align*}
	\dP{w_i}{u_j} = \dP{}{u_j}\frac{u_i}{\|u\|} = \frac{1_{i=j}-\frac{u_j}{\|u\|^2}u_i}{\|u\|}
	\end{align*}
	And so multiplying and dividing the matrix by $g^2=\|w\|^2$ gives us
	\begin{align*}
		\nabla_u L &= \frac{g}{\|u\|} \left( I- \frac{ww^T}{\|w\|^2} \right) \nabla_w L \\
		&= s W^* \nabla_w L
	\end{align*}
	Where $s=\frac{g}{\|w\|}$ and $W^* = \left( I- \frac{ww^T}{\|w\|^2} \right)$ is a projection matrix that projects onto the complement of $w$.
	\item The effect in the figure is a consequence of (b). Let $u' = u + \alpha\nabla_u L$, which is standard gradient descent with $\alpha$ learning rate. Since $W^*$ projects $\nabla_u L$ orthogonal to $w$, we have that $ w \perp \nabla_u L$ and so $ u \perp \nabla_u L$ (since $u \propto w$). Let $c=\frac{\|\nabla_u L\|}{\|u\|}$. We can use Pythagorean theorem (due to the orthogonality of $u$ and $\nabla_u L$) to get that
	\begin{align*}
	\|u'\| = \| u + \alpha\nabla_u L \| &= \sqrt{\|u\|^2+\alpha^2\|\nabla_u L\|^2}\\
	&= \sqrt{\|u\|^2+\alpha^2c^2\|u\|^2}\\
	&= \sqrt{1+\alpha^2c^2}\|u\|\\
	&\ge \|u\|
	\end{align*}
	We see that $\|u\|$ grows monotonically, and this growth is proportional to $\alpha$. This explains what is happening in the graph.
\end{enumerate}

\newpage

\bibliographystyle{ieeetr}
\bibliography{A2.bib}


\end{document}